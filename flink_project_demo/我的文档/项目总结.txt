2022.05.23
1、通过 Flink 设置检查点保存的位置若为文件系统，若保存在 HDFS 中，由 Flink程序访问 HDFS ，用户为：Administrator
    对文件路径没有写的权限，所以需要：
        方式一） env.getCheckpointConfig().setCheckpointStorage("hdfs://hadoop202:8020/gmall/ck");
                  //2.7 设置操作hdfs的用户
                  System.setProperty("HADOOP_USER_NAME","atguigu");
            将当前操作 hadoop 的用户设置为有权限写的用户
        方式二） hdfs dfs -chmod -R 777 /
            将 HDFS 中的所有文件的权限都改为 777

2、设置状态后端的时候 ：
    2.1 设置状态存储的位置   : env.setStateBackend(new HashMapStateBackend());
    2.2 设置检查点存储的位置 : env.getCheckpointConfig().setCheckpointStorage("hdfs://hadoop102:8020/gmall/ck");
            注意           : 当通过 Flink 将检查点存储到 HDFS 时，操作的用户时 Administer，没有相应的写权限。

    Flink 的状态后端：
        Flink 1.12 时，存在三种状态后端
            MemoryStateBacked   基于内存的状态后端，状态存储在TaskManager 中，检查点保存在 JobManager 中
            FsStateBacked       基于文件系统的状态后端，状态存储在TaskManager ，检查点存储在文件系统中
            RocksDBStateBacked  基于RocksDBStateBacked ,状态存储在 RocksDB，检查点存储在远程文件系统中
        Flink 1.13 时，存在两种状态后端
            HashMapStateBacked          ： 同 1.12 中的 内存和文件系统，状态都存储在 TaskManager 中，可以设置保存检查点的路径
            EmbeddedRocksDBStateBacked ：  同 1.12 RocksDBStateBacked

3、Kafka 消费者的反序列化：
        new SimpleStringSchema():
            new FlinkKafkaConsumer<String>(topic, new SimpleStringSchema(), props);
            此种反序列化的方式会导致 为null 的值，抛出异常
        new KafkaDeserializationSchema<String>() {} ：
            new FlinkKafkaConsumer<>(topic, new KafkaDeserializationSchema<String>() {}, props);
            此种方式可以设置对于 null 的值，如何进行反序列化，一般直接返回 null

4、lombok 注解
        可以简化开发，但是一般在公司时候需要使用，还是需要看一下其他同事是否在用，然后再评估是否使用。
        @Data : 可以使我们编写的 JavaBean 少些很多代码，会自动添加 getter/setter，toString ， equals
        @AllArgsConstructor : 提供全参构造器
        @NoArgsConstructor  : 提供无参构造器

5、Flink CDC 连接问题：
        方案1：在cdc连接的代码那 你加一个 .debeziumProperties(props) Properties props = new Properties().setProperty("jdbc.properties.useSSL","false")
        方案2：试试这个方案 https://blog.csdn.net/wuyu7448/article/details/121131352
        方案3：把flinkCDC版本改低
           https://ververica.github.io/flink-cdc-connectors/release-1.4/content/about.html#usage-for-datastream-api

6、算子状态：（就两种）
        广播状态
        列表状态

7、对于函数式接口（接口中只有一个抽象方法）的使用，可以使用
        匿名函数类
        lambda 表达式
        方法默认调用： 类名::方法名

8、对于 connect 连接， 必须 主流.connect(广播流)

9、通过 FastJson 通过 key 获取一个 .getString(), 获取到的 string ，如果需要校验该String 是否为 json 格式的 string，
        需要调用 FastJson 的 JSONValidator.from(dataJsonObj.toJSONString()).validate();

10、广播状态：
        BroadcastState<String, TableProcess> broadcastState =  ctx.getBroadcastState(mapStateDescriptor);
             ctx 为环境上下文对象
        broadcastState.put(sourceTable, tableProcess)

12、将数组转换成 List ：
        String[] arr = new String[5];
        List<String> list = Arrays.asList(arr)

11、移除 map 中的某个元素，如果满足某个条件
        List<String> columnList = Arrays.asList(columns);
        Set<Map.Entry<String, Object>> entries = dataJsonObj.entrySet();
        entries.removeIf(entry -> !columnList.contains(entry.getKey()));

12、Hbase 开启集群： start-hbase.sh

13、Phoenix 开启 客户端：sqlline.py

14、将流转换为 广播流：
        MapStateDescriptor<String, TableProcess> mapStateDescriptor = new MapStateDescriptor<>("dim_prop", String.class, TableProcess.class);
        BroadcastStream<String> dimPropStream = mysqlDS.broadcast(mapStateDescriptor);
        广播流中，广播状态描述器 mapStateDescriptor 是一个单例实现

15、单例设计模式：
        饿汉式： 类加载时就会创建一个单例的对象
        懒汉式： 只有其他类调用当前单例类所提供的创建对象的方法时，单例对象才会被调用 （数据库连接池等更多使用才方式，降低资源的消耗）
                 双重校验锁 ，解决懒汉式单例模式线程安全问题

16、一般对于 synchronized 关键字设置线程锁，有
        同步方法
            同步方法会对当前方法所有的代码都上锁，效率比较低。
        同步代码块
            建议使用同步代码块，只对一部分代码上锁

17、集合中删除元素，使用遍历集合本身的方式，会报错，而使用迭代器，通过迭代器删除，则不会报错
        错误写法：
            // 通过 集合自身遍历的方式删除元素
            Set<Map.Entry<Integer, String>> entries = map.entrySet();
            for (Map.Entry<Integer, String> entry : entries) {
                if (entry.getKey() == 1) {
                    entries.remove(entry);
                }
            }
        正确写法：
            // 通过 iterator 迭代器方式删除元素
            Iterator<Map.Entry<Integer, String>> iterator = entries.iterator();
            while (iterator.hasNext()) {
                Map.Entry<Integer, String> next = iterator.next();
                if (next.getKey() == 1) {
                    iterator.remove();
                }
            }

18、检查点相关的设置
        env.enableCheckpointing(5000L, CheckpointingMode.EXACTLY_ONCE);
            其中CheckpointingMode.EXACTLY_ONCE
                指的是 检查点分界线 barrier
                1、如果是 EXACTLY_ONCE  ，则表名所有的检查点分界线都到齐，才会执行当前算子状态快照，精确一次的状态一致性
                2、如果是 AT_LEAST_ONCE , 则表明至少 有一个 检查点到了后，就会保存当前算子状态快照，至少一次，效率高，但是精确性差


19、Flink 项目中，如果读取的是 Kafka 中的数据，
       从消费角度考虑   ----- 那么设置并行度需要与 Kafka 中对应 topic 的分区数相同，使得每个分区一个消费者。整个并行度构成一个消费者组。
       从水位线角度考虑 ----- 如果分区数为2，读取并行度设置为4， 后续keyBy如果设置4个分区，那么就有两个分区一直在等待水位线的到来，就会一直处于等待状态
                            后续的开窗、定时任务操作则无法触发


2022.05.24
1、StringUtils.join(Collection col, Seperator sep) : 将当前集合中的元素，按照指定分隔符进行拼接
    类似 concat_ws()
    List<Integer> list = new ArrayList<>(1,2,3,4);  ---> 1,2,3,4  没有最外层的 ()

2、对于将dim 层的数据写入到对应的表中，直接从 JSONObj value ，中获取 key,value 往 insert 语句中插入，
    可能存在 key 与 value 顺序不一致 的问题，但是当前 JSONObject 内部是一个个的 EntrySet ，从 这个里
    获取 Key ，value ，取出来的顺序就是可以保证的。
String upsertSQL = "upsert into " + GmallConfig.PHOENIX_SCHEMA + "." + sink_table + " ("
                + StringUtils.join(value.keySet(), ",") + ")"
                +  " values ('"
                + StringUtils.join(value.values(), "','") + "')" ;

3、使用 Connection 的 子类 PhoenixConnection 连接的话，需要手动提交事务，因为其 autoCommit=false
    所以使用 PhoenixConnection 创建的 Connection ，需要执行完 SQL ，自己提交sql语句
    而项目中使用的是 DruidDataSource，数据库连接池，其事务是自动提交的。

4、历史唯独数据的处理
在通过 maxwell 采集维度数据的时候，maxwell 只会按照配置读取的库，
去读取mysql 中的相应库的binlog， 将变化的数据采集到 kafka 中，而历史的维度数据
现在是没法采集到的。 而 maxwell 提供的maxwell-bootstrap 功能，可以将历史全表数据
进行扫描，然后交给 maxwell 分装为相应的 json字符串，然后再发送到 kafka中

5、流量域事务事实表 ：
    1）将flume 采集到的日志服务器数据存入 Kafka 中的 topic_log 主题中
    2）Flink 程序从 topic_log 中读取日志数据
    3）根据流量域日志数据的不同分类：
            启动日志    : dwd_traffic_start_log
            页面日志    : dwd_traffic_page_log
            曝光日志    : dwd_traffic_display_log
            动作日志    : dwd_traffic_action_log
            错误日志    : dwd_traffic_error_log
       分别写入对应的 topic 中
       主题命名方式 ： dwd_数据域_种类_log
    4) 由于可能存在用户清理了缓存，is_new 新用户状态被清理掉了，导致新老访问
        在后续指标统计时出错，所以当前需要使用 Flink 状态编程，将新用户的状态
        保存起来

 6、Flink 中的 分区、分组、分流
    分区：设置不同的并行度，就代表这分不同的区。 例如 kafka 中topic 有 4 个分区，当前Flink 设置 4 个并行度，那么就分了 4个区
    分组： keyBy ,按照 key 的hash  % 分区个数取余， 可能存在多个 key 进入一个分区的可能，所以分区的范围更大
    分流： 通过侧输出流进行分流。 OutputTag 定义流的分支，在主流中通过 SideOutputStream（tag） 获取到分支流。

7、侧输出流的用途：
    1） 数据延迟，对于超过水位线的数据，还有窗口的 allowedLateness 做保证，而窗口最后都关了，则后续还有数据到来，则会将依然迟到的数据，存放到侧输出流中
    2） 数据分流 ： 给定不同的 OutputTag，从而将不同标记的数据，发往不同的流中

8、ali- FastJson 使用
    1） TableProcess tableProcess = jsonObject.getObject("after", TableProcess.class);
        通过给定类，在 JsonObject 中，获取 给定 key 所对应的 value（Json格式的） ，将value 转换成 这个给定类。
        如果 value（Json）中的 key 的名称与 给定类的属性一一对应
      或如果 value（Json）中的 key 的名称以下划线分隔，拼接起来与给定类的属性一一对应
      都可以将 value 转换成 给定类
    2）JSONObject afterJsonObj = jsonObj.getJSONObject("after");
       TableProcess tb = afterJsonObj.toJavaObject(TableProcess.class);
       效果同 1）
    3） JSONObject js = JSON.parseObject（JsonStr）;
        将 json格式的字符串转换成 JSONObject

9、各种套路：
    1）检查点的设置套路
        开启检查点
        设置超时时间
        设置两个检查点之间的最小时间间隔
        Job取消时，检查点是否保存
        重启策略
        状态后端
        检查点存储位置
        HDFS 操作用户设置为 有权限读写的用户

    2）JDBC执行sql套路
        注册驱动
        获取链接
        获取数据库操作对象
        执行sql
        处理结果集
        释放资源

    3）Kafka 消费者套路
       指定 topic 名称
       指定 消费者组
       key 的反序列化
       value 的反序列化
       Kafka集群地址

    4）侧输出流编写的套路
       定义侧输出流标签： OutputTag opt = new OutputTag("error_log"){}；
       主流调用process 算子：
       在 process 算子中，通过 运行时上下文 ctx.output(opt, data),将数据写入到侧输出流中
       主流.getSideOutput(opt) 获取侧输出流

10、OutputTag 声明的注意点； 匿名内部类
       OutputTag opt = new OutputTag<String>("error_log"){}； // 匿名内部类的方式，泛型不会被擦除
       OutputTag opt = new OutputTag<String>("error_log")；   // 本类方式，泛型会被擦除

11、Kafka 篇：
    生产者粘性分区切换的条件：
        batch.size=16kb
        linger.ms=0s
    FlinkKafkaProducer:
        两阶段提交： 将数据往下游写，一开始标记为预提交，等检查点保存完成后，才会真的提交。 可以保证 精准一次性（Exactly-Once）
         如： public class FlinkKafkaProducer<IN>  extends TwoPhaseCommitSinkFunction
         1）由于FlinkKafkaProducer 是两阶段提交的。
            当数据由生成者发送到对应的topic中时，对于消费者一开始是不可见的（由于两阶段提交）。如果设置了检查点。
            检查点从 source -> transform -> sink，当检查点分界线（barrier）到达sink后，会对sink 中的状态进行保存，
            当检查点存储好后（存储在状态后端设置的方式，检查点存储位置），数据才会被真正提交。然后才会进行第二次提交。开启新的事务

            正是因为FlinkKafkaProducer是两阶段提交的，为了保证提交的 ACID，所以肯定涉及到事务。而当前这个事务又和检查点有关，
            检查点超时时间设置的是 1分钟，那么事务提交的时间就应该比一分钟久（但最大15min）
            所以在进行生产的时候，指定 Properties时，需要
                props.setProperty(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG,15*60*1000 + "")
         2) new FlinkKafkaProducer时，有多种传参方式，有的构造器传参不是Exactly-Once 的，例如：
                public FlinkKafkaProducer(
                            String topicId,
                            SerializationSchema<IN> serializationSchema,
                            Properties producerConfig) {}
                底层是：Semantic.AT_LEAST_ONCE,
                    semantic：语义的

        配置参数： ProducerConfig
    FlinkKafkaConsumer:
        配置参数： ConsumerConfig


2022.05.25
1、获取到 FlinkKafkaProducer 后，也将数据传入到 其构造器中，需要通过  流.addSink(producer) 才可以。

2、状态：
    算子状态： 一般在 source 端或者 sink端使用
              执行某个算子，每个并行度保存一个算子状态
              例如：FlinkKafkaConsumer 每个并行度消费一个分区的数据，消费到当前分区哪里了，需要在自己本地的Slot 中保存这样一个消费到的状态
        广播状态
        列表状态

       使用场景：
            当前项目中，使用 FlinkKafkaConsumer，维护偏移量，使用到了算子状态
            将配置流进行广播，使用到了广播状态
    键控状态：

3、is_now=0, 但是键控状态为null ，如果用户删除了访问记录，is_now=1,键控状态为null ，就得将这次的时间更新到键控状态中

4、不能在声明的时候直接对状态进行初始化，因为这个时候还获取不到 RuntimeContext, 只有在 open（）生命周期方法被调用的时候，才会有 RuntimeContext

5、Kafka 生产者保证精准一次性：
    ack=-1     isr队列>=2  , isr 中活跃的 副本>=2
    幂等性 : 单会话、单分区 的幂等性
    事务

DAU: daily Active User    日活用户
TTL: Time To Live         失效时间


2022.05.25
1、Flink 是如何保证 Exactly-Once 的状态一致性的 ？
    两阶段提交 + 状态保存
    以 FlinkKafkaProducer 为例。 当数据到达 Sink 时，会开启事务，数据从 sink 发往 Kafka 的 topic 中时，会先标记为 预提交状态，这时消费者是消费不到这个数据的
    我们设置了检查点，检查点从source 开始传递，经过 transform 到 sink ，当检查点分界线 barrier 到 sink后，会将状态进行备份。备份完毕后，才会进行第二次提交
    提交完毕后，其他消费者才可以看到数据。 然后开启第二次的事务，进行下一个数据的提交

2、SimpleDateFormatter 存在线程安全问题，因为底层会对 Calender 进行修改。

3、Flink CEP : 当使用一个数据进行处理的时候，无法完成功能，而需要正对一个模板中的多个数据同时处理，
               才能完成需求，则需要使用到 Flink CEP
    Flink CEP 开发流程：
        1、定义 Pattern
        2、应用到流上
        3、从流中提取数据
        (对于 设置了模板，给定了 withIn（）,两个数据之间的时间间隔后，如果第一个数据到了，超过了withIn)
        给定的时间后，第二个还没到，则将超时的数据放在侧输出流中

TimedOutPartialMatchHandler

4、对于状态的 TTL，是在状态描述器中进行设置的。具体设置如下
    new RichFilterFunction<JSONObject>() {
                    private ValueState<String> lastVisitDateState;
                    @Override
                    public void open(Configuration parameters) throws Exception {
                        ValueStateDescriptor<String> valueStateDescriptor = new ValueStateDescriptor<>("lastVisitDateState", String.class);
                        valueStateDescriptor.enableTimeToLive(
                            StateTtlConfig.newBuilder(Time.days(1))
                                //.setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)    // 默认方式
                                //.setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)   // 默认方式
                                .build()
                        );
                        lastVisitDateState = getRuntimeContext().getState(valueStateDescriptor);
                    }
    应该可以说 状态的很多设置都是通过状态描述器完成的，而 对于 ValueState 来说，只能 .value（） 来获取状态; .update() 来更新状态。

2022.05.26
1、interval join
    1） 语法：
          empWM
              .keyBy(emp -> emp.getDeptno())
              .intervalJoin(deptWM.keyBy(dept -> dept.getDeptno()))
              .between(Time.seconds(-5), Time.seconds(5))
              .process();
    2） 底层：
            connect （） +  维护了两条流的状态
         数据到来先判断是否迟到，如果不是迟到了，先将数据放入缓存中，然后使用当前流的元素与另一条流缓存的元素来关联
         缺点，只能实现 inner join
    3）实现的功能： inner join
    4）时间范围内的数据的状态都会被维护起来。

2、Flink SQL :
    1)操作步骤：
        流执行环境
            StreamExecutionEnvrioment env = StreamExecutioEnvrioment.getExecutionEnviroment();
        设置并行度
            env.setParalliem(1);
        流中表的环境：
                StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);
        将流转换为动态表：
                tableEnv.createTemporaryView("emp",empDS);

         //默认历史状态永不失效，如果需要设置失效时间，则可以按照如下方式设置
         tableEnv.getConfig().setIdleStateRetention(Duration.ofSeconds(5));

    2）连接方式
      普通join ： 维护两个状态
        inner join      ： 状态更新类型: UpdateType  左表： OnCreateAndWrite       右表：OnCreateAndWrite
            默认会将历史状态 的数据进行保存，然后进行内联接

        left outer join ：  状态更新类型: UpdateType  左表： OnReadAndWrite         右表：OnCreateAndWrite
            如果左边中的数据对应的右边中的数据没有，则此时只有左边的数据，左表数据标记为 +I
            当右表的数据出现后，左表中的数据会进行回撤，称之为回撤流（rewrite），回撤数据被标记为+D
            然后会将左表和右表进行连接，此时标记为 +I

        right outer join :  状态更新类型: UpdateType  左表： OnCreateAndWrite        右表：OnReadAndWrite

        tableEnv.getConfig().setIdleStateRetention(Duration.ofSeconds(10));
        设置状态保存时间，默认情况下，使用普通连接，状态会永久保存在内存中，但是很多场景下，历史状态数据是不需要一直保存的，尤其实时计算

      LookUp ： 不会维护状态
        1、专门用于进行维度关联的连接方式。 将事实表作为左表，需要维度退化的表作为右表，然后左表提供一个处理时间
        2、可以直接读取 mysql 中的数据
        3、在将通过 LookUp 读取到的维度表数据 与实时表进行关联的时候，需要在实时表中存在一个处理时间的字段。 proc_time as proctime()
        4、并且在进行关联的时候，需要指示出 事实表中处理时间字段，因为一般维度表的数据不会频繁发生改变，而实时表中的数据则是不断变化的。
        我们希望得到事实表最新的数据对应的维度信息。
            如 通过 LookUp 读取到 mysql 中的 dept（维度），通过 kafka 读取到的 emp（实时表），将两张表进行关联的时候
            select
                e.empno,
                e.ename,
                e.deptno,
                d.dname
            from emp e
            join dept for system_time as of e.proc_time d
            on e.deptno = d.deptno;
        官方解释： LookUp 通常用于使用从外部系统查询的数据来丰富表。LookUp要求一个表具有处理时间属性，而另一个表由查找源连接器支持。
                  LookUp 使用上面的处理时间临时连接语法和由查找源连接器支持的正确表

    3）将表中的数据写入到 Kafka
        步骤：
           Kafka的方式：适合于作为消费者使用
                1）添加 Flink SQL的依赖
                2）创建一张表，通过Kafka 的方式 （Flink 官网中去查）
                    CREATE TABLE KafkaTable (
                      `user_id` BIGINT,
                      `item_id` BIGINT,
                      `behavior` STRING,
                      `ts` TIMESTAMP(3) METADATA FROM 'timestamp'
                    ) WITH (
                      'connector' = 'kafka',
                      'topic' = 'user_behavior',
                      'properties.bootstrap.servers' = 'localhost:9092',
                      'properties.group.id' = 'testGroup',
                    )
                3） tableEnv.executeSQL()
                    如果通过 insert into table select * from A left outer join B ;
                        由于当右表中的数据不存在时，左表数据存在，下次右表数据过来后，会有 Delete 操作
                      'scan.startup.mode' = 'earliest-offset',
                      'format' = 'csv'
                        那么对于当前这种方式就是不合适的。
                        会报错： doesn`t support consuming update and delete changes which is produced by node Join
                4) 在从 Kafka 中消费数据，然后将消费的数据创建为 Flink SQL 中的表的时候，需要将 消费到的数据的各个属性隐射为 创建的
                   Kafka 动态表的各个字段。这时就需要将 表中的字段要与消费到得数据的属性一样，表中的字段去找相同的消费到数据的属性，进行匹配
                   消费到的数据属性，可以比表中的字段数据多
                5）对于消费到的数据为 Json 格式，如果json 中某个属性的数据又是 Json ，此时需要将 表中对应字段的数据类型设置为 map 结构，
                   可以查看官网 中 开发 - 数据类型
                   map<String, String>

           UpsertKafka的方式：适合于作为生产者使用
                作为生产者时，创建对应的表，必须提供主键，否则会报：
                    ‘upsert-kafka' table require to define a PRIMARY KEY constraint.

    4）如果读取的是 mysql 数据库中的数据，将mysql 中的数据作为 Flink SQL中的表的数据
        可以通过 FlinkSQL 提供的 JDBC方式连接 Mysql ，除此之外，还提供
            PostgreSQL
            Derby
        LookUp 缓存是用来提升有 JDBC 连接器参与的 时态关联性能的。默认情况下，缓存是没有开启的，所有请求都会被发往数据库
        当缓存启用时，每个进程（即 TaskManager）维护一份缓存。收到请求时，Flink 会先查询缓存，如果缓存未命中才会向外部数据库发送请求，
        并用查询结果更新缓存。如果缓存中的记录条数达到了 lookup.cache.max-rows 规定的最大行数时将清除存活时间最久的记录。
        如果缓存中的记录存活时间超过了 lookup.cache.ttl 规定的最大存活时间，同样会被清除

        如 emp 作为事实表，需要包含一个处理时间：
            CREATE TABLE emp (\n" +
                "  empno BIGINT,\n" +
                "  ename string,\n" +
                "  deptno bigint,\n" +
                "  proc_time as proctime()\n" +     // 处理时间可以通过 proc_time as proctime() 函数进行获取
                ")

        通过 lookUp 方式进行关联查询
        SELECT
            e.empno,e.ename,e.deptno,d.dname
        FROM
            emp e
        JOIN
            dept FOR SYSTEM_TIME AS OF e.proc_time d   // 使用 LookUp时，需要指定一个处理时间给对应的事实表，也就是让维度表通过 对应的处理时间去找事实表
        ON e.deptno = d.deptno

    5）对于 Flink SQL 来说，SQL本身是可以 提交的，所以 env.execute() 可以省略

    6）当执行了 查询语句，获取到 Table 对象，此时如果直接通过
        tableEnv.executeSql("select * from cart_add");  --- 此种方式会将 tableName 当成变量名，而不是表名
        经过 Flink SQL 连续查询后，得到的依然是动态表，如果想要 Flink 知道查询的结果是动态表，需要进行注册，
        注册的两种方式：
            方式一：直接使用 sql 拼接 上 Table cart_add = tableEnv.sqlQuery();
               tableEnv.executeSql("select * from " + cart_add);
               这种方式会调用 Table cart_add 的toString 方法，而 Table 的toString 方法内部会将 cart_add 注册为 Flink SQL 中的一张表
            方式二：
                tableEnv.registTable(cart_add) ; // 这种方式过时了，使用 createTemporaryView 进行替换（）
                tableEnv.createTemporaryView("cart_add", cart_add);

    7) tableEnv.executeSql() 直接执行 sql ，不需要进行注册
       tableEnv.sqlQuery() : 得到的是 Table 变量，需要进行注册

2022.05.28
1、FlinkSQL 中的 Join 操作
    1、Flink 原生API 中提供 ： interval join ----- 只能实现 inner join ，功能比较局限
    2、Flink SQL提供的join ：  -- 维护了两个状态 UpdateType（enum） -- Disabled, OnReadAndWrite, OnCreateAndWrite
        完成 FlinkSql ，基本的操作 步骤：
             前提：添加相对应的依赖
            // 1.1 流执行环境
            StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
            // 1.3 将 流执行环境创建为 流表环境
            StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);
            // 获取数据源
            DataStream empDS = env.socket("hadoop102", 8888);
            // 将数据源创建为临时视图
            tableEnv.createTemporaryView("emp" , empDS);
            注意：1）通过流创建的动态表就可以直接使用了。
                        tableEnv.createTemporaryView("emp", empDS);
                    可以直接 tableEnv.executeSql("select * from emp");
                 2) tableEnv.executeSql();后并不能直接打印在控制台，依然需要 调用 print() 这个下沉算子
                    tableEnv.executeSql().print();  // FlinkSql 会自动提交执行，不用 env.execute()
    支持的基本 join ：维护两个状态
        inner join        :
            左表： OnReadAndWrite
            右表：  OnReadAndWrite
        left outer join   :
            左表： OnReadAndWrite
            右表： OnCreateAndWrite
        right outer join  :
            左表： OnCreateAndWrite
            右表： OnReadAndWrite
        full outer join   :
           左表： OnReadAndWrite
           右表：  OnReadAndWrite
            tableEnv.executeSql("select e.empno, e.ename, d.deptno, d.deptname from emp e full join dept d on e.deptno = d.deptno")
               .print();
       注：1）FlinkSQL中 默认的状态是一直有效的，但是某些情况，对于状态我们希望是有时效性的，
            可以通过： tableEnv.getConfig().setIdleStateRetention()  来设置状态的有效期
           2）FlinkSQL 中 executeSql 会自动调用程序执行，所以不需要再 env.execute();

2、执行 FlinkSQL ： tableEnv.executeSql(sqlStr);
    注意 ： sqlStr 这个sql 字符串中不能有 分号 “ ; ” , 否则会报错
    org.apache.calcite.sql.parser.SqlParseException: Encountered ";"

3、查询缓存（LookUp Cache）方式
    为什么会有 LookUp 方式？
        LookUp 缓存是用来提升有 JDBC 连接器参与的 时态关联性能的。
        默认缓存方式时没有开启的，会去 JDBC所连接的数据库进行查询。
        当开启了后，先查缓存，没有结果再查 数据库

        JDBC 连接器可以作为时态表关联中的查询数据源（又称维表）。目前，仅支持同步查询模式

    如何开启 LookUp 缓存方式？
        默认情况下，查询缓存（Lookup Cache）未被启用，需要设置 ， 可以在 维度表中添加这两个配置
            lookup.cache.max-rows
            lookup.cache.ttl 参数来启用此功能。

            "CREATE TABLE dept (\n" +
                            "  deptno bigint,\n" +
                            "  dname string,\n" +
                            "  ts bigint\n" +
                            ") WITH (\n" +
                            "   'connector' = 'jdbc',\n" +
                            "   'url' = 'jdbc:mysql://hadoop102:3306/gmall2022_config',\n" +
                            "   'table-name' = 't_dept',\n" +
                            "   'username' = 'root',\n" +
                            "   'password' = 'root',\n" +
                            "   'lookup.cache.max-rows' = '200',\n" +
                            "   'lookup.cache.ttl' = '1 hour',\n" +
                            "   'driver'='com.mysql.cj.jdbc.Driver'" +
                            ")");

4、LookUp Join
    通常在 Flink SQL 表和外部系统查询结果关联时使用。

    这种关联要求一张表（主表）有处理时间字段，而另一张表（维表）由 Lookup 连接器生成。
    	Lookup Join 做的是维度关联，而维度数据是有时效性的，那么我们就需要一个时间字段来对数据的版本进行标识。因此，Flink 要求我们提供处理时间用作版本字段。
    	此处选择调用 PROCTIME() 函数获取系统时间，将其作为处理时间字段。

    	inner join dept for system_time as of e.proc_time d;

    注意事项：
        1、事实表需要提供 proc_time
        2、连接查询的时候， 需要在 开启了 LookUp 的表 中 ：
            inner join dept for system_time as of e.proc_time d;

2022.05.29
1、 macros 宏命令

1、Flink 中的自定义函数？
    标量函数---- UDF
    表值函数---- UDTF
    聚合函数---- UDAF

    对于分词场景，需要使用 表值函数---UDTF ,步骤如下
        1、继承  org.flink.apache.table.functions.TableFunction  类， 实现  eval（） 方法

2、电商业务知识
    1）搜索后的页面 ： good_list 商品列表页面

3、Flink SQL
    炸裂：
        select keyword, rowtime from filterTable, Lateral table(ik_analyze(fullword)) t(keyword)
    获取系统当前时间：
        unix_timestamp() : 获取 时间戳的
        current_row_timestamp() : 获取时间的 yyyy-MM-dd HH:mm:ss.xxx
        proctime() :获取时间的 yyyy-MM-dd HH:mm:ss.xxx
    将 flink SQL 中的表转换为 流：  (因为 Flink SQL 没有提供将 sql 的数据写入到 ClickHouse 中的支持，但是Flink 支持将流数据写入到 ClickHouse 中)
        table.toAppendStream()  ： 如果只是追加操作，则使用追加流
        table.toRetractStream() ：如果会对数据进行修改，使用回撤流
    转换为流的过程中，肯定也需要将查询的结果封装为对应的实体类，通过实体类封装每一个查询结果

4、Flink 往其他的输出端输出，如何保证 精确一次？
    Kafka ： 两阶段提交 + 事务 +  检查点机制
    Phoenix ：幂等性
    ClickHouse ： replacingMergeTree 可以帮助做去重操作
                  replicateMergeTree 保证数据不丢失（副本表）

5、遵循 JDBC规范的数据库
    Mysql
    Phoenix
    ClickHouse

ClickHouse 专题：
1、ClickHouse如何保证幂等性？？

2、ClickHouse 分片？

3、ClickHouse 副本？

4、ClickHouse 是否可以保证精准一次性？
    严格来说，保证不了，但是可以通过一些手段保证 ClickHouse 的精准一次性
    1、创建表的时候，可以使用
        副本引擎（ReplicateMergeTree）--》 可以保证成批数据过来后的一致性，
          如第一次 A,B,C,D ;第二次也是 A,B,C,D，那么第二次的 A,B,C,D 就会被过滤掉。
          但如果第二次是， A,E,F,G; 那么对于 A数据来说，是没法进行去重的。
          可以整个批次进行过滤
    2、创建表的时候，指定了 副本引擎（ReplicateMergeTree）后，同时指定去重引擎（ReplacingMergeTree）
           虽然插入数据的时候不能立即去重，但是
                可以在查询的时候，通过 optimize 来手动触发进行去重
           但有更好的方式：可以在查询的时候，在查询语句后 加上 finall

    总结：建表的时候 ReplicateMergeTree 、ReplacingMergeTree
         查询的时候 加上 finall
         这样就可以保证精确一次性啦

2022.05.30
学习 ClickHouse内容，
笔记见：E:\WorkSpace\BigData_Review_Adam\clickhouse_demo\src\我的文档\ClickHouse总结.txt

2022.05.31
1、consignee ：收货人
              英式:[ˌkɒnsaɪˈniː];美式:[ˌkɑːnsaɪˈniː]

2、current_row_timestamp（） ： flinkSql 中获取某一行的插入时间

3、sql 中指定 watermark ？ 如何指定，什么时候指定？
    肯定是在创建表的时候指定 wm ，如何指定，那就从官网中查看 建表语句，看看建表语句中是如何指定 wm 的

4、为什么 OLAP 选用的是 ClickHouse ？
    １、提供 的是 SQL 语法， 上手容易  （类比 ES 的 SDL 简直太有好了）
    2、列式存储，分析起来快速
    3、性能是目前 OLAP 中最高的， 优于 Impala , Kylin ,因为 ClickHouse 即使是查询单条sql ，也会占用全部 的 CPU资源
    4、分片、副本
    5、提供了丰富的引擎，建表的时候保存在什么位置，支不支持切片，索引

2022.06.01
1、Flink SQL 中的 timestamp 为：
    Data type of a timestamp without time zone consisting of year-month-day hour:minute:second[.fractional] with up to nanosecond precision
    and values ranging from 0000-01-01 00:00:00.000000000 to 9999-12-31 23:59:59.999999999.

2、FlinkSQL 中的几种时间函数：
    unix_timestamp()         : 返回当前时间戳
    current_row_timestamp()  : 返回 'yyyy-MM-dd HH:mm:ss' 的时间
    proctime()               : 返回 'yyyy-MM-dd HH:mm:ss' 的时间

3、将数据往 JDBC 类的数据库中写的时候，都可以使用 JDBCSink 这个类
    1）为什么维度数据 通过 JDBCSink 往 Phoenix 中写？？
        JDBCSink 只能将数据写入到一张表中，而我们的维度数据表众多

4、JDBC 这里，所有的索引都是从 1 开始

2022.06.02
1、有没有自定义过 FlinkSQL 中的 UDF/UDTF/UDAF ???
     有过， 在完成 页面关键词统计的时候，我们通过自定义 UDTF 实现了 将用户搜索的内容拆分成多个 keyword ，通过分组、开窗、聚合，对 keyword 进行统计

2022.06.03
1、Flink SQL 中：
    1）指定水位线：建表的时候指定
        CREATE TABLE Orders (
            `user` BIGINT,
            product STRING,
            order_time TIMESTAMP(3),
            WATERMARK FOR order_time AS order_time - INTERVAL '5' SECOND
        ) WITH ( . . . );
    【
        当前项目中，对于 DwsTrafficSourceKeywordPageViewWindow 流量域页面关键词聚合的需求，创建的表为：
        create table if not exists dwd_traffic_page_log(
        	`common`	map<string, string>,
        	`page`	    map<string, string>,
        	`ts` 	    bigint,
        	order_time as TO_TIMESTAMP(from_unixtime(ts)),
            WATERMARK FOR order_time AS order_time - INTERVAL '10' SECOND
        )
        如上所示，其中
            TIMESTAMP(3) 中的 3 为精度， 格式为：yyyy-MM-dd HH:mm:ss 格式
            对于当前项目，提取到的 ts 为 bigInt 类型，
            ts bigint --> timestamp :
                    1、ts --> 日期字符串：          需要调用 from_unixTime(second[, string]) 将 秒值的数据转换成stringg 类型的 string
                    2、日期字符串 -->timestamp:     to_timestamp(string1, string2) 将string1 按照 string2 的格式进行转换
    】
    2）分组后开窗：
        CREATE TABLE Orders (
          user       BIGINT,
          product    STIRNG,
          amount     INT,
          order_time TIMESTAMP(3),
          WATERMARK FOR order_time AS order_time - INTERVAL '1' MINUTE
        ) WITH (...);

        SELECT
          user,
          TUMBLE_START(order_time, INTERVAL '1' DAY) AS wStart,
          SUM(amount) FROM Orders
        GROUP BY
          TUMBLE(order_time, INTERVAL '1' DAY),
          user

2、静态方法，其泛型模板声明加到类的后面是找不到的，所以需要按照如下操作：
    public static <T> SinkFunction<T> getJdbcSink(String sql)
    其中第一个<T> 表示泛型模板声明

2022.06.04
1、package org.apache.commons.lang3; 下提供了对 String 操作的便捷的工具类：StringUtils
    1）判断对象为空     ： StringUtils.isEmpty(str);
    2) 判断对象不为空   ： StringUtils.isNotEmpty(str);



































